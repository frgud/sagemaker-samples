{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0aa55c4",
   "metadata": {},
   "source": [
    "# Simple SageMaker Sample: XGBoost Hyperparameter Tuning, Training & Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73859b45",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preparation](#Preparation)\n",
    "3. [Get the Data](#Download-and-prepare-the-data)\n",
    "4. [Simple Model Training](#Perform-a-simple-training-job)\n",
    "5. [Hyperparameter Optimization Model Training](#Hyper-parameter-tuning)\n",
    "6. [Deploy Model](#Deploy-our-model)\n",
    "7. [Local Deploy and Testing](#Local-deploy-and-testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041cef3",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook walks through building a SageMaker model using the SageMaker Python APIs.  We are using Tensorflow as the framework that drives our training jobs.  The samples here have been copied and modified from the various examples and notebooks published by AWS on the AWS GitHub site.\n",
    "\n",
    "For a deeper dive into next steps on SageMaker model deployment and monitoring, check out this full day self-paced [Immersion Day](https://sagemaker-immersionday.workshop.aws/en/prerequisites.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271adab4",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies.\n",
    "!pip install zipfile38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from time import gmtime, strftime\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "# Define Session Variables\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_client = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Set the S3 we will use for this, replace with the value you will use for this lab.\n",
    "bucket = \"BUCKET-NAME-HERE\"\n",
    "prefix = \"bank-additional\"\n",
    "\n",
    "# Request the algorithm image & version from this region\n",
    "container = retrieve(\"xgboost\", region, \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dda023",
   "metadata": {},
   "source": [
    "## Download and Prepare the Data\n",
    "In this section we will prepare our training, validation, and test data sets for our model.  There are several steps in this section that we will perform by hand using numpy and pandas.  However, all of these steps can be automated in [SageMaker Studio using DataWrangler](https://sagemaker-immersionday.workshop.aws/lab1/option1.html).  \n",
    "\n",
    "The sections in the data preparation piece of this lab are as follows:\n",
    "1. Download the data set\n",
    "2. Perform preliminary analysis\n",
    "3. Feature Engineering\n",
    "4. Shuffle, Split & Store to S3\n",
    "\n",
    "The example data set we are using is designed to predict if customers of a bank will enroll for a CD (certificate of deposit).  The data set has been sourced from the University of California, Irvine, and a version of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/bank+marketing).\n",
    "\n",
    "#### Input Variables\n",
    "##### Bank Client Data:\n",
    "1. **age** (numeric)\n",
    "2. **job** : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. **marital** : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4. **education** (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5. **default**: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6. **housing**: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7. **loan**: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "##### Related to the Last Contact of the Current Campaign:\n",
    "8. **contact**: contact communication type (categorical: 'cellular','telephone')\n",
    "9. **month**: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10. **day_of_week**: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11. **duration**: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "##### Other Features:\n",
    "12. **campaign**: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13. **pdays**: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14. **previous**: number of contacts performed before this campaign and for this client (numeric)\n",
    "15. **poutcome**: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "\n",
    "##### Socieconomic Context Features:\n",
    "16. **emp.var.rate**: employment variation rate - quarterly indicator (numeric)\n",
    "17. **cons.price.idx**: consumer price index - monthly indicator (numeric)\n",
    "18. **cons.conf.idx**: consumer confidence index - monthly indicator (numeric)\n",
    "19. **euribor3m**: euribor 3 month rate - daily indicator (numeric)\n",
    "20. **nr.employed**: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "#### Output Cariable (desired target)\n",
    "1. **y** - has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761310e",
   "metadata": {},
   "source": [
    "### Download the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://sagemaker-sample-data-us-east-2.s3-us-east-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n",
    "    \n",
    "with zipfile.ZipFile('bank-additional.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv')\n",
    "pd.set_option('display.max_rows', 20)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee530520",
   "metadata": {},
   "source": [
    "### Perform Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c740a7",
   "metadata": {},
   "source": [
    "Here we will plot observations from our dataset.  The goal here is to understand how our dataset looks and what features correlate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "    \n",
    "# Histograms for each numeric feature\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f994ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    if column != 'y':\n",
    "        display(pd.crosstab(index=data[column], columns=data['y'], normalize='columns'))\n",
    "        \n",
    "for column in data.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = data[[column, 'y']].hist(by='y', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f96419",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716afea",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Here we perform some feature engineering.  First, we define a column that describes if a customer has been contacted once before.  Then, we conflate employment information to describe if a person is working or not.  Then we perform One-Hot Encoding on our categorical features.  Finally, we drop a few columns that we have already discerned are inconsequential to column `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)\n",
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)\n",
    "model_data = pd.get_dummies(data)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e544b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.drop(columns=['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'])\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93d68b",
   "metadata": {},
   "source": [
    "### Shuffle, Split, Store in S3\n",
    "This is a straightforward process of partitioning our dataset to be used to train, validate, and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    model_data.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(model_data)), int(0.9 * len(model_data))],\n",
    ")\n",
    "\n",
    "print(\"Training Data Length ==> \" + str(len(train_data.columns)))\n",
    "print(\"Validation Data Length ==> \" + str(len(validation_data.columns)))\n",
    "print(\"Test Data Length ==> \" + str(len(test_data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900feb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_csv = pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=True)\n",
    "pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['y_no', 'y_yes'], axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c329597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85894853",
   "metadata": {},
   "source": [
    "## Perform a Simple Training Job\n",
    "\n",
    "In this section we'll discuss the various methods of training models in SageMaker.  We are using XGBoost as the avatar for this discussion.  There are several ways to train models in SageMaker; refer to the following outline for guidance.\n",
    "\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html#xgboost-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"verbosity\": \"2\",\n",
    "    \"silent\": \"0\",\n",
    "}\n",
    "\n",
    "job_name = \"BlueOceanID-xgboost-spot-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 3600 if use_spot_instances else None\n",
    "checkpoint_s3_uri = (\n",
    "    \"s3://{}/{}/checkpoints/{}\".format(bucket, prefix, job_name) if use_spot_instances else None\n",
    ")\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)\n",
    "\n",
    "xgb_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "#     instance_count=4,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    volume_size=5,  # GB\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    ")\n",
    "train_input = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/{}\".format(bucket, prefix, \"train/train.csv\"), content_type=\"csv\"\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/{}\".format(bucket, prefix, \"validation/validation.csv\"), content_type=\"csv\"\n",
    ")\n",
    "xgb_model.fit({\"train\": train_input, \"validation\": validation_input}, job_name=job_name)\n",
    "xgb_model.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf5c917",
   "metadata": {},
   "source": [
    "## Deploy Our Model\n",
    "In this section we will discuss two seperate deployment paradigms.  We will deploy our model using a SageMaker hosted endpoint first, followed by a local deployment.  The steps for local deployment can be reused to deploy to a machine in a private or self-hosted environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we deploy the simple XGBoost model we trained earlier...\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor = xgb_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a070a8",
   "metadata": {},
   "source": [
    "### Test our Deployed Model\n",
    "\n",
    "In this section we'll utilize the testing data we created earlier on the model we deployed to a SageMaker endpoint.\n",
    "\n",
    "Now, we’ll use a simple function to: \n",
    "1. Loop over our test dataset \n",
    "2. Split it into mini-batches of rows \n",
    "3. Convert those mini-batchs to CSV string payloads \n",
    "4. Retrieve mini-batch predictions by invoking the XGBoost endpoint \n",
    "5. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = \"\"\n",
    "    for array in split_array:\n",
    "        predictions = \",\".join([predictions, predictor.predict(array).decode(\"utf-8\")])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbca18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_data.to_numpy()[:, 1:])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index=test_data.iloc[:, 0],\n",
    "    columns=np.round(predictions),\n",
    "    rownames=[\"actual\"],\n",
    "    colnames=[\"predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86574379",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6db1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d9280",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "\n",
    "Hyperparameter Tuning is a feature of Amazon SageMaker that allows customers to find the best hyperparameter values for their training jobs in the least amount of time.  This feature is available in the SageMaker UI within the AWS Console, the AWS CLI, and via the Python SageMaker API.  We will describe how to submit a HTJ (Hyperparameter Tuning Job) via API calls in this document.\n",
    "\n",
    "The steps to submit an HTJ via the SageMaker Python API are as follows:\n",
    "1. Decide which algorithm you want to tune.\n",
    "2. Set initial weights on the hyperparameters for the selected algorithm.\n",
    "3. Define the range of hyperparameters you want to tune against.\n",
    "4. Define your hyperparameter tuning job.\n",
    "5. Train your hyperparameter tuning job.\n",
    "6. Evaluate the your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ce2f3",
   "metadata": {},
   "source": [
    "### Decide which algorithm to tune\n",
    "Amazon SageMaker Hyperparameter Tuning allows you to tune built-in SageMaker algorithms as well as custom algorithms that you bring to the training environment.  To complete this process, you need to pull the model container using the `sagemaker.image_uris.retrieve` function call.  This function takes a model name, region and version number as parameters and returns a container object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Create a reference to an XGBoost model with the retrieved image\n",
    "# The API call tells SageMaker what the algorithm is, what permissions it has, \n",
    "#   what kind and how many instances to train with, where output should do,\n",
    "#   and the session credentials for this algorithms.\n",
    "hyper_xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f9d01",
   "metadata": {},
   "source": [
    "### Set Initial Weights on Hyperparameters\n",
    "\n",
    "Every algorithms has different hyperparameters.  Before you run a training job for hyperparameter tuning you need to set default weights on the algorithm for these hyperparameters.  During this phase we will also set the objective metric for the HTJ.\n",
    "\n",
    "The objective metric is a metric used by Amazon SageMaker to compare the performance of hyperparameter tuning jobs against each other.  The model with the best performing metric is the model that should be used for training.\n",
    "\n",
    "Every algorithm will have different hyperparameters.  It is important to understand what these hyperparameters are and they work at a high level before running an HTJ.  [XGBoost Hyperparameters & Objective Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html) are listed in the AWS Documentation for Amazon SageMaker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial weights on XGBoost\n",
    "hyper_xgb.set_hyperparameters(\n",
    "    eval_metric=\"auc\",\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=10,\n",
    "    rate_drop=0.3,\n",
    "    tweedie_variance_power=1.4,\n",
    ")\n",
    "\n",
    "# Set the Objective Metric to compare job runs.\n",
    "# This validation metric is Area Under the Curve.\n",
    "objective_metric_name = \"validation:auc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5a1a0",
   "metadata": {},
   "source": [
    "### Define the range of Hyperparameters you want to Tune\n",
    "\n",
    "Amazon SageMaker Hyperparameter Tuning Jobs require a specific range of values to use when testing hyperparameters.  You need to specify these ranges as `sagemaker.parameter.ParameterRange` objects in a dictionary.  This class is implemented in the `sagemaker.tuner` package with implementations for `IntegerParameter`, `CategoricalParameter` & `ContinuousParameter`.  In this example, we'll tune continuous parameters.\n",
    "\n",
    "When specifying a parameter to tune, you pass a minimum value, a maximum value, and a scaling type.  Amazon SageMaker Hyperparameter Tuning Jobs support Linear and Logarithmic scaling for parameters.  Test both and see which perform best for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8386f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges_logarithmic = {\n",
    "    \"alpha\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "}\n",
    "\n",
    "hyperparameter_ranges_linear = {\n",
    "    \"alpha\": ContinuousParameter(0.01, 10, scaling_type=\"Linear\"),\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Linear\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629e885",
   "metadata": {},
   "source": [
    "### Random vs. Bayesian; Linear vs. Logarithmic \n",
    "\n",
    "There are four types of hyperparameter tuning jobs you can run, we will showcase them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/{}\".format(bucket, prefix, \"train/train.csv\"), content_type=\"csv\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=\"s3://{}/{}/{}\".format(bucket, prefix, \"validation/validation.csv\"), content_type=\"csv\"\n",
    ")\n",
    "\n",
    "letters = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ff076",
   "metadata": {},
   "source": [
    "### Submit the Hyperparameter Tuning Job\n",
    "\n",
    "A Hyperparameter Tuning Job is represented in the SageMaker API as a `HyperparameterTuner` object.  To create this object we must pass in many of the objects we've created up until this point.  We supply the algorithm we will tune, the objective metric, and the hyperparameter ranges.  Furthermore, we will set the number of jobs to run and the parallelization of these jobs.  \n",
    "\n",
    "Finally, we must specify our tuning strategy.  Hyperparameter Tuning Jobs take a `Random` or `Bayesian` strategy, and we will showcase both in this example.\n",
    "\n",
    "The `fit(dict, bool)` method submits your Hyperparameter Tuning Job to Amazon SageMaker.  The hardware specified in the algorithm definition is initialized and the jobs are run in accordance with the instructions defined within the `HyperparameterTuner` job.\n",
    "\n",
    "The hyperparameter ranges we've specified will be tested and modified across all of the jobs that run; these values will change based on the scaling strategy you utilize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c92a7",
   "metadata": {},
   "source": [
    "#### Random Linear Scaling Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8262e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Hyperparameter Tuning Job that scales Linearly.\n",
    "random_linear_tuner_log = HyperparameterTuner(\n",
    "    hyper_xgb,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges_linear,\n",
    "    max_jobs=10, max_parallel_jobs=4,\n",
    "    strategy=\"Random\",\n",
    ")\n",
    "\n",
    "suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "random_linear_tuner_log.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    include_cls_metadata=False,\n",
    "    job_name=\"random-linear-tuner-\" + suffix\n",
    ")\n",
    "random_linear_tuner_log.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac423d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_linear_status_log = client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=random_linear_tuner_log.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_linear_df_log = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    random_linear_tuner_log.latest_tuning_job.job_name\n",
    ").dataframe()\n",
    "random_linear_df_log[\"strategy\"] = \"random|linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e236f7d",
   "metadata": {},
   "source": [
    "#### Random Logarithmic Scaling Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Hyperparameter Tuning Job that scales Logarithmically.\n",
    "random_logarithmic_tuner_log = HyperparameterTuner(\n",
    "    hyper_xgb,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges_logarithmic,\n",
    "    max_jobs=10, max_parallel_jobs=4,\n",
    "    strategy=\"Random\",\n",
    ")\n",
    "\n",
    "suffix = ''.join(random.choice(letters) for i in range(7))\n",
    "random_logarithmic_tuner_log.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    include_cls_metadata=False,\n",
    "    job_name=\"random-logarithmic-tuner-\" + suffix\n",
    ")\n",
    "random_logarithmic_tuner_log.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_logarithmic_status_log = client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=random_logarithmic_tuner_log.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dbc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_logarithmic_df_log = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    random_logarithmic_tuner_log.latest_tuning_job.job_name\n",
    ").dataframe()\n",
    "random_logarithmic_df_log[\"strategy\"] = \"random|logarithmic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b372a0",
   "metadata": {},
   "source": [
    "#### Bayesian Linear Scaling Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Search Hyperparameter Tuning Job that scales Linearly.\n",
    "bayesian_linear_tuner_log = HyperparameterTuner(\n",
    "    hyper_xgb,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges_linear,\n",
    "    max_jobs=10, max_parallel_jobs=4,\n",
    "    strategy=\"Bayesian\",\n",
    ")\n",
    "\n",
    "suffix = ''.join(random.choice(letters) for i in range(10))\n",
    "bayesian_linear_tuner_log.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    include_cls_metadata=False,\n",
    "    job_name=\"bayesian-linear-tuner-\" + suffix\n",
    ")\n",
    "bayesian_linear_tuner_log.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66900ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_linear_status_log = client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=bayesian_linear_tuner_log.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_linear_df_log = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    bayesian_linear_tuner_log.latest_tuning_job.job_name\n",
    ").dataframe()\n",
    "bayesian_linear_df_log[\"strategy\"] = \"bayesian|linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681a42e",
   "metadata": {},
   "source": [
    "#### Bayesian Logarithmic Scaling Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9968a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Search Hyperparameter Tuning Job that scales Logarithmically.\n",
    "bayesian_logarithmic_tuner_log = HyperparameterTuner(\n",
    "    hyper_xgb,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges_logarithmic,\n",
    "    max_jobs=10, max_parallel_jobs=4,\n",
    "    strategy=\"Bayesian\",\n",
    ")\n",
    "\n",
    "suffix = ''.join(random.choice(letters) for i in range(5))\n",
    "bayesian_logarithmic_tuner_log.fit(\n",
    "    {\"train\": s3_input_train, \"validation\": s3_input_validation},\n",
    "    include_cls_metadata=False,\n",
    "    job_name=\"bayesian-logarithmic-tuner-\" + suffix\n",
    ")\n",
    "bayesian_logarithmic_tuner_log.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aef6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_logarithmic_status_log = client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=bayesian_logarithmic_tuner_log.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bfd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_logarithmic_df_log = sagemaker.HyperparameterTuningJobAnalytics(\n",
    "    bayesian_logarithmic_tuner_log.latest_tuning_job.job_name\n",
    ").dataframe()\n",
    "bayesian_logarithmic_df_log[\"strategy\"] = \"bayesian|logarithmic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3d667",
   "metadata": {},
   "source": [
    "### Evaluate the results\n",
    "\n",
    "Using third party libraries we plot the performance of the hyperparameter tuning jobs in a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5809ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Create the Pandas dataframe that plots the performance of our models #\n",
    "########################################################################\n",
    "\n",
    "# HOW DO LAMBDA AND ALPHA PLAY TOGETHER WITH XGBOOST?\n",
    "\n",
    "# REMINDER: As Alpha and Lambda increase, the model becomes more conservative.\n",
    "# Since we are using AUC as the objective metric, we can assume smaller values will be preferred.\n",
    "\n",
    "df = pd.concat([\n",
    "    random_linear_df_log,\n",
    "    random_logarithmic_df_log,\n",
    "    bayesian_linear_df_log,\n",
    "    bayesian_logarithmic_df_log\n",
    "], ignore_index=True)\n",
    "\n",
    "g = sns.FacetGrid(df, col=\"strategy\", palette=\"viridis\")\n",
    "g = g.map(plt.scatter, \"alpha\", \"lambda\", alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6b6eb",
   "metadata": {},
   "source": [
    "This concludes our segment on Hyperparameter Tuning.  The next task is to select the model we want to work with and deploy that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd4284",
   "metadata": {},
   "source": [
    "### Deploy Our Hyperparameter Trained Model\n",
    "In this section we will discuss two seperate deployment paradigms.  We will deploy our model using a SageMaker hosted endpoint first, followed by a local deployment.  The steps for local deployment can be reused to deploy to a machine in a private or self-hosted environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we deploy the best of 1 of the 4 models...\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "hyper_predictor = random_linear_tuner_log.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513dac45",
   "metadata": {},
   "source": [
    "### Test our Deployed Hyperparameter Model\n",
    "\n",
    "In this section we'll utilize the testing data we created earlier on the model we deployed to a SageMaker endpoint.\n",
    "\n",
    "Now, we’ll use a simple function to: \n",
    "1. Loop over our test dataset \n",
    "2. Split it into mini-batches of rows \n",
    "3. Convert those mini-batchs to CSV string payloads \n",
    "4. Retrieve mini-batch predictions by invoking the XGBoost endpoint \n",
    "5. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_model_predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = \"\"\n",
    "    for array in split_array:\n",
    "        predictions = \",\".join([predictions, hyper_predictor.predict(array).decode(\"utf-8\")])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = hyperparameter_model_predict(test_data.to_numpy()[:, 1:])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    index=test_data.iloc[:, 0],\n",
    "    columns=np.round(predictions),\n",
    "    rownames=[\"actual\"],\n",
    "    colnames=[\"predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b265724",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
